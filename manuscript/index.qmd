# Defining challenges and benchmarking open problems in Single-cell analysis 

### Abstract

### Introduction

Single-cell genomics has enabled the study of biological processes at an
unprecedented scale and
resolution^1--3^. These
studies have been enabled by computational tools for the analysis of
single-cell data, which has surpassed 1300 published
algorithms^4^. Evolving
biological questions and technological
development^5,6^
continuously pose new challenges for data analysts, which has led to the
description of grand challenges for single-cell data
science^7^. While the
identification of such grand challenges in single-cell genomics can
concentrate research into relevant areas, the current definitions of
these challenges are qualitative in nature.

Clear definitions of open problems including quantifications of progress
has driven innovation in machine learning
research^8^. Open challenges such
as the Netflix prize and
ImageNet^9^ represent landmark
problems that allow measurement of progress across decades. These
challenges have provided a common vocabulary for innovation: image
classification accuracy has improved from 50% to over 90% on ImageNet
since 2011. Inspired by open challenges in machine learning, Critical
Assessment of protein Structure Prediction (CASP), Recursion's cellular
image classification Kaggle competition, various DREAM challenges, and
our recent multimodal data integration competition at NeurIPS
2021^10^ have focused research
efforts in computational biology disciplines such as protein structure
prediction, cellular diagnostics, gene module prediction, and multimodal
single-cell data integration. Particularly successful competitions such
as CASP have built communities that accelerate research toward these
goals.

In single-cell genomics, large-scale
benchmarks^11--14^
have quantified open questions by proposing evaluation metrics and using
these to compare existing methods. Yet, benchmarks inevitably age: newly
developed tools that optimize the proposed benchmarking metrics cannot
be compared in the same study; existing tools may perform differently on
newly generated benchmarking datasets of better quality; and used
metrics may not capture the aspects of method performance that a user is
interested in. These challenges can only be addressed by community
participation in benchmarking. To build such communities around
quantified challenges, benchmarking studies would require frequent
updating and open discussion on benchmarking datasets, proposed metrics,
and selected methods.

Learning from machine learning competitions, we propose a living
benchmarking framework for single-cell genomics tools that follows the
Common Task Framework (CTF)^8^.
The CTF has 3 criteria: (1) An easily available training dataset of
sufficient complexity, (2) a set of competitors who submit methods for
benchmarking, and (3) a set of metrics that are evaluated on test data
to define progress on the task and thereby formally define it. When
continuously updating such benchmarks, ...

We have developed the Open Problems in Single-cell Analysis (Open
Problems) framework, a platform that defines and measures progress
towards open challenges in single-cell genomics following the CTF. Open
Problems combines an open github repository with community-defined
tasks, Github Actions testing workflows, a benchmarking pipeline using
Nextflow and Amazon Web Services, and a website to explore the results
to build a living, community-based benchmarking project. Currently, Open
Problems includes X defined tasks, on which Y datasets are used to
evaluate Z methods using XY metrics. These tasks were defined by
interactions between contributors, which has led to new benchmarking
insights even when contributors had already published benchmarks on
these tasks. Open Problems provides community-defined standards for
progress in single-cell data science to enable decentralized method
development towards a common goal.

### An infrastructure for living benchmarks in Single-cell Data Science

In order to enable a truly living benchmark, we have designed a
standardized and automated infrastructure which allows members of the
single-cell community to contribute to Open Problems in a seamless
manner. Each Open Problems task is comprised of datasets, methods, and
metrics (Fig 1): datasets define both the input and the ground truth for
a task, methods attempt to solve the task, and metrics evaluate the
success of a method on a given dataset. Metrics are normalized between 0
and 1 based on the inclusion of "baseline" methods which are designed to
emulate either random or perfect performance. Methods are then ranked on
a per-dataset basis by the average normalized metric score and presented
in a summary table on the Open Problems website
([[https://openproblems.bio]{.ul}](https://openproblems.bio)).

To enable seamless community involvement in Open Problems, we have
designed the infrastructure to take advantage of automated workflows
through GitHub Actions, Nextflow\[cite\], and AWS Batch. When a
community member adds a task, dataset, method, or metric, the new
contributions are automatically tested on the cloud. When all tests pass
and the new contribution is merged into the main repository, the results
from the new contribution are automatically submitted to the Open
Problems website. To maximize reproducibility, all code is run within
Docker containers and all data is downloaded from public repositories,
including figshare, GEO, and CELLxGENE\[cite\]. Task definitions,
choices of metrics, and implementations of methods care discussed on our
github repository
([[www.github.com/openproblems-bio/openproblems]{.ul}](http://www.github.com/openproblems-bio/openproblems))
and can be easily amended by pull requests which are reviewed by task
leaders (who initially defined the task) and the core infrastructure
team.

### Community-defined open problems in Single-cell analysis

Building on previous work defining open challenges in single-cell
analysis^7^ and many independent
benchmarking studies in single-cell
genomics^12,13,15--23^,
we defined X Open Problems tasks (Fig 2a). While some tasks were
directly transferred from published benchmarking papers (e.g., batch
correction^12^), others were
defined directly by method developers in the single-cell community
(e.g., spatial decomposition). To exemplify how such a task is defined
and the value that the task adds, we elaborate on the spatial
deconvolution task.

1.  Task overview

2.  Results from a few tasks

    a.  All of the [[scIB]{.ul}](https://github.com/theislab/scIB)
        > benchmarking can be migrated into this framework (highly
        > extensible)

    b.  Addition of completely novel tasks (e.g. gene regulatory
        > prediction)

    c.  Community-driven benchmarking for emerging methodology (e.g.
        > differential abundance, spatial decomposition)

    d.  Results of the CZI jamboree

3.  Focus on spatial decomposition task

    a.  Explain metrics, datasets, methods

### Defined open problems facilitate/drive innovation in single-cell data science

1.  Reaching out to the ML community:

    a.  NeurIPS competition built on Open Problems

        i.  Note: Community-informed re-evaluation of metrics (PMLR
            > report)

2.  Using framework to drive development of novel methods

3.  Enable method developers to include their methods in live benchmark
    > and publish results

4.  Build analysis pipelines from top performers across tasks

### Acknowledgements

-   CZI: Jonah Cool, Ivana Williams, Fiona Griffin

-   Mo Lotfollahi for early discussions

### References

1\. Plass, M. *et al.* Cell type atlas and lineage tree of a whole
complex animal by single-cell transcriptomics. *Science* vol. 360
eaaq1723 (2018).

2\. Cao, J. *et al.* A human cell atlas of fetal gene expression.
*Science* **370**, (2020).

3\. Montoro, D. T. *et al.* A revised airway epithelial hierarchy
includes CFTR-expressing ionocytes. *Nature* **560**, 319--324
(2018).

4\. Zappia, L. & Theis, F. J. Over 1000 tools reveal trends in the
single-cell RNA-seq analysis landscape. *Genome Biol.* **22**, 301
(2021).

5\. Method of the Year 2020: spatially resolved transcriptomics. *Nat.
Methods* **18**, 1 (2021).

6\. Method of the Year 2019: Single-cell multimodal omics. *Nat.
Methods* **17**, 1 (2020).

7\. LÃ¤hnemann, D. *et al.* Eleven grand challenges in single-cell data
science. *Genome Biol.* **21**, 31
(2020).

8\. Donoho, D. 50 Years of Data Science. *Journal of Computational and
Graphical Statistics* vol. 26 745--766
(2017).

9\. Deng, J. *et al.* Imagenet: A large-scale hierarchical image
database. in *2009 IEEE conference on computer vision and pattern
recognition* 248--255 (Ieee, 2009).

10\. Luecken, M. D. *et al.* A sandbox for prediction and integration
of DNA, RNA, and proteins in single cells.
(2021).

11\. Saelens, W., Cannoodt, R., Todorov, H. & Saeys, Y. A comparison of
single-cell trajectory inference methods. *Nat. Biotechnol.* **37**,
547--554 (2019).

12\. Luecken, M. D. *et al.* Benchmarking atlas-level data integration
in single-cell genomics. *Nat. Methods* **19**, 41--50
(2022).

13\. Soneson, C. & Robinson, M. D. Bias, robustness and scalability in
single-cell differential expression analysis. *Nat. Methods* **15**,
255--261 (2018).

14\. Squair, J. W. *et al.* Confronting false discoveries in
single-cell differential expression. *Nat. Commun.* **12**, 5692
(2021).

15\. Li, B. *et al.* Benchmarking spatial and single-cell
transcriptomics integration methods for transcript distribution
prediction and cell type deconvolution. *Nature Methods* (2022)
doi:[10.1038/s41592-022-01480-9](http://dx.doi.org/10.1038/s41592-022-01480-9).

16\. Hou, W., Ji, Z., Ji, H. & Hicks, S. C. A systematic evaluation of
single-cell RNA-sequencing imputation methods. *Genome Biol.* **21**,
218 (2020).

17\. Raimundo, F., Vallot, C. & Vert, J.-P. Tuning parameters of
dimensionality reduction methods for single-cell RNA-seq analysis.
*Genome Biol.* **21**, 212 (2020).

18\. Sun, X., Lin, X., Li, Z. & Wu, H. A comprehensive comparison of
supervised and unsupervised methods for cell type identification in
single-cell RNA-seq. *Brief. Bioinform.* **23**,
(2022).

19\. Sun, S., Zhu, J., Ma, Y. & Zhou, X. Accuracy, robustness and
scalability of dimensionality reduction methods for single-cell RNA-seq
analysis. *Genome Biol.* **20**, 269
(2019).

20\. Huang, Y. & Zhang, P. Evaluation of machine learning approaches
for cell-type identification from single-cell transcriptomics data.
*Brief. Bioinform.* **22**, (2021).

21\. Chazarra-Gil, R., van Dongen, S., Kiselev, V. Y. & Hemberg, M.
Flexible comparison of batch correction methods for single-cell RNA-seq
using BatchBench. *Nucleic Acids Res.* **49**, e42
(2021).

22\. Cantini, L. *et al.* Benchmarking joint multi-omics dimensionality
reduction approaches for cancer study.
doi:[10.1101/2020.01.14.905760](http://dx.doi.org/10.1101/2020.01.14.905760).

23\. Avila Cobos, F., Alquicira-Hernandez, J., Powell, J. E., Mestdagh,
P. & De Preter, K. Benchmarking of cell type deconvolution pipelines for
transcriptomics data. *Nat. Commun.* **11**, 5650
(2020).

